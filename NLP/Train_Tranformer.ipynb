{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b1a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Crochet_Project\\chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4426cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>colors</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want a shinchan plushie</td>\n",
       "      <td>plushie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>shinchan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make a hello kitty doll</td>\n",
       "      <td>doll</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hello kitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crochet panda plushie around 25 cm</td>\n",
       "      <td>plushie</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small bunny plushie 18 cm</td>\n",
       "      <td>plushie</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want a teddy bear plushie 30 cm with 3 colors</td>\n",
       "      <td>plushie</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>teddy bear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  product  height width  \\\n",
       "0                        I want a shinchan plushie  plushie     0.0     0   \n",
       "1                          Make a hello kitty doll     doll     0.0     0   \n",
       "2               Crochet panda plushie around 25 cm  plushie    25.0     0   \n",
       "3                        Small bunny plushie 18 cm  plushie    18.0     0   \n",
       "4  I want a teddy bear plushie 30 cm with 3 colors  plushie    30.0     0   \n",
       "\n",
       "   colors    character  \n",
       "0     0.0     shinchan  \n",
       "1     0.0  hello kitty  \n",
       "2     0.0            0  \n",
       "3     0.0            0  \n",
       "4     3.0   teddy bear  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/Train.csv\")\n",
    "df = df.fillna(0)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1935e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"product\"] = df[\"product\"].astype(str)\n",
    "df[\"character\"] = df[\"character\"].astype(str)\n",
    "\n",
    "product_encoder = LabelEncoder()\n",
    "character_encoder = LabelEncoder()\n",
    "\n",
    "df[\"product_enc\"] = product_encoder.fit_transform(df[\"product\"])\n",
    "df[\"character_enc\"] = character_encoder.fit_transform(df[\"character\"])\n",
    "\n",
    "features = df[[\"product_enc\",\"height\",\"width\",\"colors\",\"character_enc\"]]\n",
    "texts = df[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbebf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokens = tokenize(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ee982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 260.79it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class CrochetTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 5)   # product, height, width, colors, character\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.bert(ids, attention_mask=mask).last_hidden_state[:,0]\n",
    "        return self.fc(x)\n",
    "\n",
    "model = CrochetTransformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3554e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[[\"product_enc\",\"height\",\"width\",\"colors\",\"character_enc\"]]\n",
    "\n",
    "features = features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "features = features.fillna(0)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features[[\"height\",\"width\",\"colors\"]] = scaler.fit_transform(\n",
    "    features[[\"height\",\"width\",\"colors\"]]\n",
    ")\n",
    "\n",
    "X_ids = tokens[\"input_ids\"]\n",
    "X_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "y = torch.tensor(features.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60f3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196f0b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 20.4508\n",
      "Epoch 2 | Loss: 20.1770\n",
      "Epoch 3 | Loss: 19.9038\n",
      "Epoch 4 | Loss: 19.6325\n",
      "Epoch 5 | Loss: 19.3648\n",
      "Epoch 6 | Loss: 19.1024\n",
      "Epoch 7 | Loss: 18.8459\n",
      "Epoch 8 | Loss: 18.5947\n",
      "Epoch 9 | Loss: 18.3477\n",
      "Epoch 10 | Loss: 18.1048\n",
      "Epoch 11 | Loss: 17.8664\n",
      "Epoch 12 | Loss: 17.6326\n",
      "Epoch 13 | Loss: 17.4034\n",
      "Epoch 14 | Loss: 17.1787\n",
      "Epoch 15 | Loss: 16.9585\n",
      "Epoch 16 | Loss: 16.7431\n",
      "Epoch 17 | Loss: 16.5332\n",
      "Epoch 18 | Loss: 16.3296\n",
      "Epoch 19 | Loss: 16.1332\n",
      "Epoch 20 | Loss: 15.9444\n",
      "Epoch 21 | Loss: 15.7637\n",
      "Epoch 22 | Loss: 15.5921\n",
      "Epoch 23 | Loss: 15.4308\n",
      "Epoch 24 | Loss: 15.2804\n",
      "Epoch 25 | Loss: 15.1412\n",
      "Epoch 26 | Loss: 15.0128\n",
      "Epoch 27 | Loss: 14.8937\n",
      "Epoch 28 | Loss: 14.7816\n",
      "Epoch 29 | Loss: 14.6752\n",
      "Epoch 30 | Loss: 14.5739\n",
      "Epoch 31 | Loss: 14.4772\n",
      "Epoch 32 | Loss: 14.3843\n",
      "Epoch 33 | Loss: 14.2951\n",
      "Epoch 34 | Loss: 14.2096\n",
      "Epoch 35 | Loss: 14.1277\n",
      "Epoch 36 | Loss: 14.0495\n",
      "Epoch 37 | Loss: 13.9752\n",
      "Epoch 38 | Loss: 13.9047\n",
      "Epoch 39 | Loss: 13.8380\n",
      "Epoch 40 | Loss: 13.7751\n",
      "Epoch 41 | Loss: 13.7158\n",
      "Epoch 42 | Loss: 13.6599\n",
      "Epoch 43 | Loss: 13.6070\n",
      "Epoch 44 | Loss: 13.5568\n",
      "Epoch 45 | Loss: 13.5092\n",
      "Epoch 46 | Loss: 13.4640\n",
      "Epoch 47 | Loss: 13.4210\n",
      "Epoch 48 | Loss: 13.3801\n",
      "Epoch 49 | Loss: 13.3411\n",
      "Epoch 50 | Loss: 13.3040\n",
      "Epoch 51 | Loss: 13.2687\n",
      "Epoch 52 | Loss: 13.2351\n",
      "Epoch 53 | Loss: 13.2032\n",
      "Epoch 54 | Loss: 13.1728\n",
      "Epoch 55 | Loss: 13.1437\n",
      "Epoch 56 | Loss: 13.1158\n",
      "Epoch 57 | Loss: 13.0890\n",
      "Epoch 58 | Loss: 13.0631\n",
      "Epoch 59 | Loss: 13.0382\n",
      "Epoch 60 | Loss: 13.0141\n",
      "Epoch 61 | Loss: 12.9909\n",
      "Epoch 62 | Loss: 12.9686\n",
      "Epoch 63 | Loss: 12.9470\n",
      "Epoch 64 | Loss: 12.9262\n",
      "Epoch 65 | Loss: 12.9060\n",
      "Epoch 66 | Loss: 12.8865\n",
      "Epoch 67 | Loss: 12.8675\n",
      "Epoch 68 | Loss: 12.8492\n",
      "Epoch 69 | Loss: 12.8314\n",
      "Epoch 70 | Loss: 12.8142\n",
      "Epoch 71 | Loss: 12.7974\n",
      "Epoch 72 | Loss: 12.7810\n",
      "Epoch 73 | Loss: 12.7651\n",
      "Epoch 74 | Loss: 12.7496\n",
      "Epoch 75 | Loss: 12.7344\n",
      "Epoch 76 | Loss: 12.7195\n",
      "Epoch 77 | Loss: 12.7050\n",
      "Epoch 78 | Loss: 12.6908\n",
      "Epoch 79 | Loss: 12.6768\n",
      "Epoch 80 | Loss: 12.6631\n",
      "Epoch 81 | Loss: 12.6496\n",
      "Epoch 82 | Loss: 12.6363\n",
      "Epoch 83 | Loss: 12.6233\n",
      "Epoch 84 | Loss: 12.6104\n",
      "Epoch 85 | Loss: 12.5978\n",
      "Epoch 86 | Loss: 12.5853\n",
      "Epoch 87 | Loss: 12.5729\n",
      "Epoch 88 | Loss: 12.5607\n",
      "Epoch 89 | Loss: 12.5487\n",
      "Epoch 90 | Loss: 12.5368\n",
      "Epoch 91 | Loss: 12.5251\n",
      "Epoch 92 | Loss: 12.5134\n",
      "Epoch 93 | Loss: 12.5019\n",
      "Epoch 94 | Loss: 12.4905\n",
      "Epoch 95 | Loss: 12.4793\n",
      "Epoch 96 | Loss: 12.4681\n",
      "Epoch 97 | Loss: 12.4570\n",
      "Epoch 98 | Loss: 12.4461\n",
      "Epoch 99 | Loss: 12.4352\n",
      "Epoch 100 | Loss: 12.4244\n",
      "Epoch 101 | Loss: 12.4137\n",
      "Epoch 102 | Loss: 12.4030\n",
      "Epoch 103 | Loss: 12.3925\n",
      "Epoch 104 | Loss: 12.3820\n",
      "Epoch 105 | Loss: 12.3716\n",
      "Epoch 106 | Loss: 12.3612\n",
      "Epoch 107 | Loss: 12.3510\n",
      "Epoch 108 | Loss: 12.3407\n",
      "Epoch 109 | Loss: 12.3306\n",
      "Epoch 110 | Loss: 12.3205\n",
      "Epoch 111 | Loss: 12.3104\n",
      "Epoch 112 | Loss: 12.3004\n",
      "Epoch 113 | Loss: 12.2905\n",
      "Epoch 114 | Loss: 12.2806\n",
      "Epoch 115 | Loss: 12.2708\n",
      "Epoch 116 | Loss: 12.2610\n",
      "Epoch 117 | Loss: 12.2513\n",
      "Epoch 118 | Loss: 12.2416\n",
      "Epoch 119 | Loss: 12.2319\n",
      "Epoch 120 | Loss: 12.2223\n",
      "Epoch 121 | Loss: 12.2127\n",
      "Epoch 122 | Loss: 12.2032\n",
      "Epoch 123 | Loss: 12.1937\n",
      "Epoch 124 | Loss: 12.1842\n",
      "Epoch 125 | Loss: 12.1748\n",
      "Epoch 126 | Loss: 12.1654\n",
      "Epoch 127 | Loss: 12.1560\n",
      "Epoch 128 | Loss: 12.1467\n",
      "Epoch 129 | Loss: 12.1374\n",
      "Epoch 130 | Loss: 12.1282\n",
      "Epoch 131 | Loss: 12.1189\n",
      "Epoch 132 | Loss: 12.1098\n",
      "Epoch 133 | Loss: 12.1006\n",
      "Epoch 134 | Loss: 12.0914\n",
      "Epoch 135 | Loss: 12.0823\n",
      "Epoch 136 | Loss: 12.0732\n",
      "Epoch 137 | Loss: 12.0642\n",
      "Epoch 138 | Loss: 12.0552\n",
      "Epoch 139 | Loss: 12.0462\n",
      "Epoch 140 | Loss: 12.0372\n",
      "Epoch 141 | Loss: 12.0282\n",
      "Epoch 142 | Loss: 12.0193\n",
      "Epoch 143 | Loss: 12.0104\n",
      "Epoch 144 | Loss: 12.0015\n",
      "Epoch 145 | Loss: 11.9926\n",
      "Epoch 146 | Loss: 11.9838\n",
      "Epoch 147 | Loss: 11.9750\n",
      "Epoch 148 | Loss: 11.9662\n",
      "Epoch 149 | Loss: 11.9574\n",
      "Epoch 150 | Loss: 11.9486\n",
      "Epoch 151 | Loss: 11.9399\n",
      "Epoch 152 | Loss: 11.9312\n",
      "Epoch 153 | Loss: 11.9225\n",
      "Epoch 154 | Loss: 11.9138\n",
      "Epoch 155 | Loss: 11.9051\n",
      "Epoch 156 | Loss: 11.8965\n",
      "Epoch 157 | Loss: 11.8878\n",
      "Epoch 158 | Loss: 11.8792\n",
      "Epoch 159 | Loss: 11.8706\n",
      "Epoch 160 | Loss: 11.8621\n",
      "Epoch 161 | Loss: 11.8535\n",
      "Epoch 162 | Loss: 11.8450\n",
      "Epoch 163 | Loss: 11.8365\n",
      "Epoch 164 | Loss: 11.8279\n",
      "Epoch 165 | Loss: 11.8195\n",
      "Epoch 166 | Loss: 11.8110\n",
      "Epoch 167 | Loss: 11.8025\n",
      "Epoch 168 | Loss: 11.7941\n",
      "Epoch 169 | Loss: 11.7856\n",
      "Epoch 170 | Loss: 11.7772\n",
      "Epoch 171 | Loss: 11.7688\n",
      "Epoch 172 | Loss: 11.7604\n",
      "Epoch 173 | Loss: 11.7521\n",
      "Epoch 174 | Loss: 11.7437\n",
      "Epoch 175 | Loss: 11.7354\n",
      "Epoch 176 | Loss: 11.7270\n",
      "Epoch 177 | Loss: 11.7187\n",
      "Epoch 178 | Loss: 11.7104\n",
      "Epoch 179 | Loss: 11.7021\n",
      "Epoch 180 | Loss: 11.6938\n",
      "Epoch 181 | Loss: 11.6856\n",
      "Epoch 182 | Loss: 11.6773\n",
      "Epoch 183 | Loss: 11.6691\n",
      "Epoch 184 | Loss: 11.6608\n",
      "Epoch 185 | Loss: 11.6526\n",
      "Epoch 186 | Loss: 11.6444\n",
      "Epoch 187 | Loss: 11.6362\n",
      "Epoch 188 | Loss: 11.6280\n",
      "Epoch 189 | Loss: 11.6199\n",
      "Epoch 190 | Loss: 11.6117\n",
      "Epoch 191 | Loss: 11.6035\n",
      "Epoch 192 | Loss: 11.5954\n",
      "Epoch 193 | Loss: 11.5873\n",
      "Epoch 194 | Loss: 11.5792\n",
      "Epoch 195 | Loss: 11.5711\n",
      "Epoch 196 | Loss: 11.5630\n",
      "Epoch 197 | Loss: 11.5549\n",
      "Epoch 198 | Loss: 11.5468\n",
      "Epoch 199 | Loss: 11.5388\n",
      "Epoch 200 | Loss: 11.5307\n",
      "Epoch 201 | Loss: 11.5227\n",
      "Epoch 202 | Loss: 11.5146\n",
      "Epoch 203 | Loss: 11.5066\n",
      "Epoch 204 | Loss: 11.4986\n",
      "Epoch 205 | Loss: 11.4906\n",
      "Epoch 206 | Loss: 11.4826\n",
      "Epoch 207 | Loss: 11.4746\n",
      "Epoch 208 | Loss: 11.4667\n",
      "Epoch 209 | Loss: 11.4587\n",
      "Epoch 210 | Loss: 11.4507\n",
      "Epoch 211 | Loss: 11.4428\n",
      "Epoch 212 | Loss: 11.4349\n",
      "Epoch 213 | Loss: 11.4269\n",
      "Epoch 214 | Loss: 11.4190\n",
      "Epoch 215 | Loss: 11.4111\n",
      "Epoch 216 | Loss: 11.4032\n",
      "Epoch 217 | Loss: 11.3953\n",
      "Epoch 218 | Loss: 11.3875\n",
      "Epoch 219 | Loss: 11.3796\n",
      "Epoch 220 | Loss: 11.3717\n",
      "Epoch 221 | Loss: 11.3639\n",
      "Epoch 222 | Loss: 11.3560\n",
      "Epoch 223 | Loss: 11.3482\n",
      "Epoch 224 | Loss: 11.3404\n",
      "Epoch 225 | Loss: 11.3326\n",
      "Epoch 226 | Loss: 11.3248\n",
      "Epoch 227 | Loss: 11.3170\n",
      "Epoch 228 | Loss: 11.3092\n",
      "Epoch 229 | Loss: 11.3014\n",
      "Epoch 230 | Loss: 11.2936\n",
      "Epoch 231 | Loss: 11.2858\n",
      "Epoch 232 | Loss: 11.2781\n",
      "Epoch 233 | Loss: 11.2703\n",
      "Epoch 234 | Loss: 11.2626\n",
      "Epoch 235 | Loss: 11.2549\n",
      "Epoch 236 | Loss: 11.2471\n",
      "Epoch 237 | Loss: 11.2394\n",
      "Epoch 238 | Loss: 11.2317\n",
      "Epoch 239 | Loss: 11.2240\n",
      "Epoch 240 | Loss: 11.2163\n",
      "Epoch 241 | Loss: 11.2086\n",
      "Epoch 242 | Loss: 11.2009\n",
      "Epoch 243 | Loss: 11.1933\n",
      "Epoch 244 | Loss: 11.1856\n",
      "Epoch 245 | Loss: 11.1779\n",
      "Epoch 246 | Loss: 11.1703\n",
      "Epoch 247 | Loss: 11.1627\n",
      "Epoch 248 | Loss: 11.1550\n",
      "Epoch 249 | Loss: 11.1474\n",
      "Epoch 250 | Loss: 11.1398\n",
      "Epoch 251 | Loss: 11.1322\n",
      "Epoch 252 | Loss: 11.1246\n",
      "Epoch 253 | Loss: 11.1170\n",
      "Epoch 254 | Loss: 11.1094\n",
      "Epoch 255 | Loss: 11.1018\n",
      "Epoch 256 | Loss: 11.0942\n",
      "Epoch 257 | Loss: 11.0867\n",
      "Epoch 258 | Loss: 11.0791\n",
      "Epoch 259 | Loss: 11.0715\n",
      "Epoch 260 | Loss: 11.0640\n",
      "Epoch 261 | Loss: 11.0565\n",
      "Epoch 262 | Loss: 11.0489\n",
      "Epoch 263 | Loss: 11.0414\n",
      "Epoch 264 | Loss: 11.0339\n",
      "Epoch 265 | Loss: 11.0264\n",
      "Epoch 266 | Loss: 11.0189\n",
      "Epoch 267 | Loss: 11.0114\n",
      "Epoch 268 | Loss: 11.0039\n",
      "Epoch 269 | Loss: 10.9964\n",
      "Epoch 270 | Loss: 10.9889\n",
      "Epoch 271 | Loss: 10.9814\n",
      "Epoch 272 | Loss: 10.9740\n",
      "Epoch 273 | Loss: 10.9665\n",
      "Epoch 274 | Loss: 10.9591\n",
      "Epoch 275 | Loss: 10.9516\n",
      "Epoch 276 | Loss: 10.9442\n",
      "Epoch 277 | Loss: 10.9367\n",
      "Epoch 278 | Loss: 10.9293\n",
      "Epoch 279 | Loss: 10.9219\n",
      "Epoch 280 | Loss: 10.9145\n",
      "Epoch 281 | Loss: 10.9071\n",
      "Epoch 282 | Loss: 10.8997\n",
      "Epoch 283 | Loss: 10.8923\n",
      "Epoch 284 | Loss: 10.8849\n",
      "Epoch 285 | Loss: 10.8775\n",
      "Epoch 286 | Loss: 10.8701\n",
      "Epoch 287 | Loss: 10.8628\n",
      "Epoch 288 | Loss: 10.8554\n",
      "Epoch 289 | Loss: 10.8480\n",
      "Epoch 290 | Loss: 10.8407\n",
      "Epoch 291 | Loss: 10.8333\n",
      "Epoch 292 | Loss: 10.8260\n",
      "Epoch 293 | Loss: 10.8187\n",
      "Epoch 294 | Loss: 10.8113\n",
      "Epoch 295 | Loss: 10.8040\n",
      "Epoch 296 | Loss: 10.7967\n",
      "Epoch 297 | Loss: 10.7894\n",
      "Epoch 298 | Loss: 10.7821\n",
      "Epoch 299 | Loss: 10.7748\n",
      "Epoch 300 | Loss: 10.7675\n",
      "Epoch 301 | Loss: 10.7602\n",
      "Epoch 302 | Loss: 10.7529\n",
      "Epoch 303 | Loss: 10.7456\n",
      "Epoch 304 | Loss: 10.7384\n",
      "Epoch 305 | Loss: 10.7311\n",
      "Epoch 306 | Loss: 10.7238\n",
      "Epoch 307 | Loss: 10.7166\n",
      "Epoch 308 | Loss: 10.7093\n",
      "Epoch 309 | Loss: 10.7021\n",
      "Epoch 310 | Loss: 10.6948\n",
      "Epoch 311 | Loss: 10.6876\n",
      "Epoch 312 | Loss: 10.6804\n",
      "Epoch 313 | Loss: 10.6732\n",
      "Epoch 314 | Loss: 10.6659\n",
      "Epoch 315 | Loss: 10.6587\n",
      "Epoch 316 | Loss: 10.6515\n",
      "Epoch 317 | Loss: 10.6443\n",
      "Epoch 318 | Loss: 10.6371\n",
      "Epoch 319 | Loss: 10.6299\n",
      "Epoch 320 | Loss: 10.6228\n",
      "Epoch 321 | Loss: 10.6156\n",
      "Epoch 322 | Loss: 10.6084\n",
      "Epoch 323 | Loss: 10.6012\n",
      "Epoch 324 | Loss: 10.5941\n",
      "Epoch 325 | Loss: 10.5869\n",
      "Epoch 326 | Loss: 10.5797\n",
      "Epoch 327 | Loss: 10.5726\n",
      "Epoch 328 | Loss: 10.5654\n",
      "Epoch 329 | Loss: 10.5583\n",
      "Epoch 330 | Loss: 10.5512\n",
      "Epoch 331 | Loss: 10.5440\n",
      "Epoch 332 | Loss: 10.5369\n",
      "Epoch 333 | Loss: 10.5298\n",
      "Epoch 334 | Loss: 10.5227\n",
      "Epoch 335 | Loss: 10.5156\n",
      "Epoch 336 | Loss: 10.5084\n",
      "Epoch 337 | Loss: 10.5013\n",
      "Epoch 338 | Loss: 10.4942\n",
      "Epoch 339 | Loss: 10.4871\n",
      "Epoch 340 | Loss: 10.4800\n",
      "Epoch 341 | Loss: 10.4730\n",
      "Epoch 342 | Loss: 10.4659\n",
      "Epoch 343 | Loss: 10.4588\n",
      "Epoch 344 | Loss: 10.4517\n",
      "Epoch 345 | Loss: 10.4447\n",
      "Epoch 346 | Loss: 10.4376\n",
      "Epoch 347 | Loss: 10.4305\n",
      "Epoch 348 | Loss: 10.4235\n",
      "Epoch 349 | Loss: 10.4164\n",
      "Epoch 350 | Loss: 10.4094\n",
      "Epoch 351 | Loss: 10.4023\n",
      "Epoch 352 | Loss: 10.3953\n",
      "Epoch 353 | Loss: 10.3882\n",
      "Epoch 354 | Loss: 10.3812\n",
      "Epoch 355 | Loss: 10.3742\n",
      "Epoch 356 | Loss: 10.3671\n",
      "Epoch 357 | Loss: 10.3601\n",
      "Epoch 358 | Loss: 10.3531\n",
      "Epoch 359 | Loss: 10.3460\n",
      "Epoch 360 | Loss: 10.3390\n",
      "Epoch 361 | Loss: 10.3320\n",
      "Epoch 362 | Loss: 10.3250\n",
      "Epoch 363 | Loss: 10.3180\n",
      "Epoch 364 | Loss: 10.3110\n",
      "Epoch 365 | Loss: 10.3040\n",
      "Epoch 366 | Loss: 10.2970\n",
      "Epoch 367 | Loss: 10.2900\n",
      "Epoch 368 | Loss: 10.2830\n",
      "Epoch 369 | Loss: 10.2760\n",
      "Epoch 370 | Loss: 10.2690\n",
      "Epoch 371 | Loss: 10.2620\n",
      "Epoch 372 | Loss: 10.2550\n",
      "Epoch 373 | Loss: 10.2480\n",
      "Epoch 374 | Loss: 10.2410\n",
      "Epoch 375 | Loss: 10.2340\n",
      "Epoch 376 | Loss: 10.2270\n",
      "Epoch 377 | Loss: 10.2200\n",
      "Epoch 378 | Loss: 10.2130\n",
      "Epoch 379 | Loss: 10.2060\n",
      "Epoch 380 | Loss: 10.1990\n",
      "Epoch 381 | Loss: 10.1920\n",
      "Epoch 382 | Loss: 10.1850\n",
      "Epoch 383 | Loss: 10.1780\n",
      "Epoch 384 | Loss: 10.1710\n",
      "Epoch 385 | Loss: 10.1640\n",
      "Epoch 386 | Loss: 10.1570\n",
      "Epoch 387 | Loss: 10.1500\n",
      "Epoch 388 | Loss: 10.1430\n",
      "Epoch 389 | Loss: 10.1360\n",
      "Epoch 390 | Loss: 10.1290\n",
      "Epoch 391 | Loss: 10.1221\n",
      "Epoch 392 | Loss: 10.1151\n",
      "Epoch 393 | Loss: 10.1081\n",
      "Epoch 394 | Loss: 10.1011\n",
      "Epoch 395 | Loss: 10.0942\n",
      "Epoch 396 | Loss: 10.0872\n",
      "Epoch 397 | Loss: 10.0802\n",
      "Epoch 398 | Loss: 10.0733\n",
      "Epoch 399 | Loss: 10.0664\n",
      "Epoch 400 | Loss: 10.0595\n",
      "Epoch 401 | Loss: 10.0525\n",
      "Epoch 402 | Loss: 10.0456\n",
      "Epoch 403 | Loss: 10.0387\n",
      "Epoch 404 | Loss: 10.0318\n",
      "Epoch 405 | Loss: 10.0250\n",
      "Epoch 406 | Loss: 10.0181\n",
      "Epoch 407 | Loss: 10.0112\n",
      "Epoch 408 | Loss: 10.0043\n",
      "Epoch 409 | Loss: 9.9975\n",
      "Epoch 410 | Loss: 9.9906\n",
      "Epoch 411 | Loss: 9.9838\n",
      "Epoch 412 | Loss: 9.9769\n",
      "Epoch 413 | Loss: 9.9701\n",
      "Epoch 414 | Loss: 9.9633\n",
      "Epoch 415 | Loss: 9.9564\n",
      "Epoch 416 | Loss: 9.9496\n",
      "Epoch 417 | Loss: 9.9428\n",
      "Epoch 418 | Loss: 9.9360\n",
      "Epoch 419 | Loss: 9.9292\n",
      "Epoch 420 | Loss: 9.9224\n",
      "Epoch 421 | Loss: 9.9156\n",
      "Epoch 422 | Loss: 9.9088\n",
      "Epoch 423 | Loss: 9.9020\n",
      "Epoch 424 | Loss: 9.8952\n",
      "Epoch 425 | Loss: 9.8884\n",
      "Epoch 426 | Loss: 9.8817\n",
      "Epoch 427 | Loss: 9.8749\n",
      "Epoch 428 | Loss: 9.8681\n",
      "Epoch 429 | Loss: 9.8614\n",
      "Epoch 430 | Loss: 9.8546\n",
      "Epoch 431 | Loss: 9.8479\n",
      "Epoch 432 | Loss: 9.8411\n",
      "Epoch 433 | Loss: 9.8344\n",
      "Epoch 434 | Loss: 9.8276\n",
      "Epoch 435 | Loss: 9.8209\n",
      "Epoch 436 | Loss: 9.8141\n",
      "Epoch 437 | Loss: 9.8074\n",
      "Epoch 438 | Loss: 9.8007\n",
      "Epoch 439 | Loss: 9.7940\n",
      "Epoch 440 | Loss: 9.7872\n",
      "Epoch 441 | Loss: 9.7805\n",
      "Epoch 442 | Loss: 9.7738\n",
      "Epoch 443 | Loss: 9.7671\n",
      "Epoch 444 | Loss: 9.7604\n",
      "Epoch 445 | Loss: 9.7537\n",
      "Epoch 446 | Loss: 9.7470\n",
      "Epoch 447 | Loss: 9.7403\n",
      "Epoch 448 | Loss: 9.7336\n",
      "Epoch 449 | Loss: 9.7269\n",
      "Epoch 450 | Loss: 9.7203\n",
      "Epoch 451 | Loss: 9.7136\n",
      "Epoch 452 | Loss: 9.7069\n",
      "Epoch 453 | Loss: 9.7003\n",
      "Epoch 454 | Loss: 9.6936\n",
      "Epoch 455 | Loss: 9.6869\n",
      "Epoch 456 | Loss: 9.6803\n",
      "Epoch 457 | Loss: 9.6736\n",
      "Epoch 458 | Loss: 9.6670\n",
      "Epoch 459 | Loss: 9.6603\n",
      "Epoch 460 | Loss: 9.6537\n",
      "Epoch 461 | Loss: 9.6470\n",
      "Epoch 462 | Loss: 9.6404\n",
      "Epoch 463 | Loss: 9.6338\n",
      "Epoch 464 | Loss: 9.6272\n",
      "Epoch 465 | Loss: 9.6205\n",
      "Epoch 466 | Loss: 9.6139\n",
      "Epoch 467 | Loss: 9.6073\n",
      "Epoch 468 | Loss: 9.6007\n",
      "Epoch 469 | Loss: 9.5941\n",
      "Epoch 470 | Loss: 9.5875\n",
      "Epoch 471 | Loss: 9.5809\n",
      "Epoch 472 | Loss: 9.5743\n",
      "Epoch 473 | Loss: 9.5677\n",
      "Epoch 474 | Loss: 9.5611\n",
      "Epoch 475 | Loss: 9.5545\n",
      "Epoch 476 | Loss: 9.5479\n",
      "Epoch 477 | Loss: 9.5413\n",
      "Epoch 478 | Loss: 9.5348\n",
      "Epoch 479 | Loss: 9.5282\n",
      "Epoch 480 | Loss: 9.5216\n",
      "Epoch 481 | Loss: 9.5150\n",
      "Epoch 482 | Loss: 9.5085\n",
      "Epoch 483 | Loss: 9.5019\n",
      "Epoch 484 | Loss: 9.4954\n",
      "Epoch 485 | Loss: 9.4888\n",
      "Epoch 486 | Loss: 9.4822\n",
      "Epoch 487 | Loss: 9.4757\n",
      "Epoch 488 | Loss: 9.4692\n",
      "Epoch 489 | Loss: 9.4626\n",
      "Epoch 490 | Loss: 9.4561\n",
      "Epoch 491 | Loss: 9.4495\n",
      "Epoch 492 | Loss: 9.4430\n",
      "Epoch 493 | Loss: 9.4365\n",
      "Epoch 494 | Loss: 9.4299\n",
      "Epoch 495 | Loss: 9.4234\n",
      "Epoch 496 | Loss: 9.4169\n",
      "Epoch 497 | Loss: 9.4104\n",
      "Epoch 498 | Loss: 9.4039\n",
      "Epoch 499 | Loss: 9.3973\n",
      "Epoch 500 | Loss: 9.3908\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    preds = model(X_ids, X_mask)\n",
    "    loss = loss_fn(preds, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9a8846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', 'alphabet', 'baby doll', 'beach', 'bear', 'bow', 'butterfly',\n",
       "       'cartoon boy', 'cat', 'floral', 'flower', 'flowers', 'heart',\n",
       "       'hello kitty', 'kids theme', 'layered', 'luxury', 'minimal',\n",
       "       'moon', 'pastel', 'pastel flowers', 'pikachu', 'princess',\n",
       "       'rainbow', 'rose', 'shinchan', 'star', 'sunflower', 'teddy bear',\n",
       "       'unicorn', 'wedding'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"crochet_transformer.pt\")\n",
    "product_encoder.classes_\n",
    "character_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6922bc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['character_encoder.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(product_encoder,\"product_encoder.pkl\")\n",
    "joblib.dump(character_encoder,\"character_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c367eef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, \"feature_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
